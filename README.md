# VoiceGPT â€“ Talk with AI in My Own Voice

Welcome to **VoiceGPT**, an AI-powered voice assistant I built that not only chats with you but **speaks back using my own cloned voice**.  
I used **Eleven Labs** to clone my voice, **FastAPI** for the backend, and a sleek **HTML/CSS frontend**. The app converts your speech to text, generates intelligent responses using **ChatGPT (OpenAI API)**, and then speaks the answers in my voice.

---

## âœ¨ Features
- **Realistic Voice Cloning** â€“ Powered by **Eleven Labs**, responses come back in my voice.
- **Speech-to-Text (STT)** â€“ Uses **Faster Whisper** for lightning-fast and accurate transcription of your audio input.
- **AI-Powered Responses** â€“ Integrated **ChatGPT (OpenAI API)** to generate intelligent and conversational replies.
- **FastAPI Backend** â€“ Handles all API calls, processing, and response generation efficiently.
- **Clean Frontend** â€“ A simple and responsive interface built with **HTML & CSS**.
- **Audio Playback** â€“ Outputs responses as real-time audio in my cloned voice.

---

## ğŸ› ï¸ Tech Stack
- **Frontend**: HTML, CSS
- **Backend**: FastAPI (Python)
- **Speech-to-Text**: Faster Whisper
- **Text Generation**: OpenAI ChatGPT API
- **Voice Cloning**: Eleven Labs
- **Audio Handling**: Python audio libraries

---

## ğŸš€ How It Works
1. **You speak** â€“ The microphone records your audio.
2. **Audio â†’ Text** â€“ Faster Whisper transcribes your voice into text.
3. **AI Thinks** â€“ The text is sent to OpenAIâ€™s ChatGPT, which generates a natural response.
4. **Text â†’ My Voice** â€“ The response is converted into audio using Eleven Labs with my cloned voice.
5. **You listen** â€“ The app plays the answer in my own voice.

---

## âš™ï¸ Setup Instructions
### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/voicegpt.git
cd voicegpt
```


### 2. Install Dependencies
Make sure you have Python 3.9+ installed, then:
```bash
pip install -r requirements.txt
```

### 3. Set Up Environment Variables
Create a `.env` file and add:
```
OPENAI_API_KEY=your_openai_api_key
ELEVEN_LABS_API_KEY=your_eleven_labs_api_key
```

### 4. Run the Backend
```bash
uvicorn main:app --reload
```

### 5. Open Frontend
Open `index.html` in your browser, or serve it with any static server.

---

## ğŸ“‚ Project Structure
```
voicegpt/
â”‚
â”œâ”€â”€ main.py            # FastAPI backend
â”œâ”€â”€ templates/         # HTML frontend
â”œâ”€â”€ static/            # CSS and JS files
â”œâ”€â”€ models/            # Faster Whisper model files
â”œâ”€â”€ uploads/           # Recorded audio files
â””â”€â”€ requirements.txt   # Python dependencies
```

---

## ğŸ¯ Goals & Vision
This project started as a fun experiment to **combine AI chat with personalized voice synthesis**.  
I wanted something that feels like a conversation with me â€” not just text on a screen.

---

## ğŸ§  What I Learned
- Integrating multiple APIs (OpenAI + Eleven Labs).
- Building a real-time voice pipeline: **Record â†’ Transcribe â†’ Respond â†’ Synthesize â†’ Play**.
- Using FastAPI for fast and clean backend architecture.
- Handling audio in Python and optimizing STT with Faster Whisper.

